
---
license: apache-2.0
language: en
datasets: wikipedia
pipeline_tag: text-generation
---

## Model description
This is a LLaMA-like model with only 68M parameters trained on Wikipedia and part of the C4-en and C4-realnewslike datasets. 

No evaluation has been conducted yet, so use it with care.

Original: https://huggingface.co/JackFram/llama-68m
